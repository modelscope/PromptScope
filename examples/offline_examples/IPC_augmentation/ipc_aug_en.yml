# use_wandb: False

task_config:
    language: 'en'
    output_path: dump
    max_usage: 6000
    task_description: Assistant is a mathematician.
    instruction: Determine whether the result of the calculation is correct. Respond with "yes" or "no".
    samples_per_step: 10
    batch_size: 10
    workers: 5

# dataset:
#     name: 'dataset'
#     records_path: null
#     initial_dataset: ''
#     label_schema: ["是", "否"]
#     max_samples: 50
#     semantic_sampling: False # Change to True in case you don't have M1. Currently there is an issue with faiss and M1

# annotator:
#     method : 'argilla'
#     config:
#         api_url: ''
#         api_key: 'admin.apikey'
#         workspace: 'admin'
#         time_interval: 5

model_config:
    generation:
        module_name: 'openai_post'
        model_name: 'gpt-4o'
        max_tokens: 2000
        seed: 1234

# annotator:
#     module_name: 'dashscope_generation'
#     model_name: 'qwen-plus'
#     clazz: 'models.llama_index_generation_model'
#     max_tokens: 2000
#     seed: 1234
    # method: 'llm'
    # config:
    #     llm:
    #         type: 'Tongyi'
    #         name: 'qwen-max'
    #     # instruction:
    #     #     '评估电影评论中是否存在剧透，如果存在请回答是，否则请回答否。'
    #     instruction:
    #         '判断计算算式的结果是否正确，回答是或者否。'
    #     num_workers: 5
    #     prompt: 'prompts/predictor_completion/prediction.prompt'
    #     mini_batch_size: 1
    #     mode: 'annotation'

# predictor:
#     module_name: 'dashscope_generation'
#     model_name: 'qwen-plus'
#     clazz: 'models.llama_index_generation_model'
#     max_tokens: 2000
#     seed: 1234


# predictor:
#     method : 'llm'
#     config:
#         llm:
#             type: 'Tongyi'
#             name: 'qwen-max'
# #            async_params:
# #                retry_interval: 10
# #                max_retries: 2
#             model_kwargs: {"seed": 220}
#         num_workers: 5
#         prompt: 'prompts/predictor_completion/prediction.prompt'
#         mini_batch_size: 1  #change to >1 if you want to include multiple samples in the one prompt
#         mode: 'prediction'

# meta_prompts:
#     folder: 'prompts/meta_prompts_classification'
#     num_err_prompt: 1  # Number of error examples per sample in the prompt generation
#     num_err_samples: 2 # Number of error examples per sample in the sample generation
#     history_length: 4 # Number of sample in the meta-prompt history
#     num_generated_samples: 10 # Number of generated samples at each iteration
#     num_initialize_samples: 10 # Number of generated samples at iteration 0, in zero-shot case
#     samples_generation_batch: 10 # Number of samples generated in one call to the LLM
#     num_workers: 5 #Number of parallel workers
#     warmup: 4 # Number of warmup steps

# eval:
#     function_name: 'accuracy'
#     num_large_errors: 4
#     num_boundary_predictions : 0
#     error_threshold: 0.5

# stop_criteria:
#     max_usage: 2 #In $ in case of OpenAI models, otherwise number of tokens
#     patience: 10 # Number of patience steps
#     min_delta: 0.01 # Delta for the improvement definition
