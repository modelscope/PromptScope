# use_wandb: False

basic_config:
    task_name: &task_name boolean_expressions
    output_path: dump
    instruction_pos: &ins_p Q_begin
    meta_prompt_type: &meta_pt both_instructions_and_exemplars
    dataset_name: &dataset bbh
    language: en

model_config:
    scorer:
        module_name: openai_aio_post
        model_name: gpt-4o
        max_tokens: 2000
        seed: 1234
        batch_size: &bs 1

    optim:
        module_name: openai_aio_post
        model_name: &optim_name gpt-4o
        max_tokens: 2000
        seed: 1234
        temperature: &temp 1
        

evolution_config:
    num_search_steps: 200
    old_instruction_score_threshold: 0.3
    extract_final_answer_by_prompting_again: False
    include_qa: False
    evaluate_in_parallel: True
    dataset_name: *dataset
    task_name: *task_name
    initial_instructions: [
        "Let's solve the problem.",
        # "",
        # "The answer is",
    ]
    instruction_pos: *ins_p
    few_shot_qa_pairs: True
    num_score_buckets: 100
    max_num_instructions: 20
    meta_prompt_type: *meta_pt
    meta_prompt_instructions_before_exemplars: True
    few_shot_selection_criteria: random
    optimizer_llm_name: *optim_name
    optimizer_llm_temperature: *temp
    num_generated_instructions_in_each_step: 8
    evaluate_generated_ins_on_few_shot: False
    num_few_shot_questions_for_instruction_refinement: 5
    evaluate_old_ins_on_few_shot: False
    eval_interval: 3
    verbose: False
    batch_size: *bs
    semaphore: 10